{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c8235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Define paths for different datasets\n",
    "RAVDESS_PATH = \"/ravdess/audio_speech_actors_01-24/\"\n",
    "CREMA_PATH = \"/cremad/AudioWAV/\"\n",
    "TESS_PATH = \"/TESS Toronto emotional speech set data/\"\n",
    "SAVEE_PATH = \"/savee/ALL/\"\n",
    "\n",
    "# RAVDESS dataset\n",
    "def load_ravdess(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "    \n",
    "    for actor_dir in directory_list:\n",
    "        actor_path = os.path.join(path, actor_dir)\n",
    "        actor_files = os.listdir(actor_path)\n",
    "        \n",
    "        for file in actor_files:\n",
    "            filename_parts = file.split('.')[0].split('-')\n",
    "            if len(filename_parts) >= 3:\n",
    "                emotion = int(filename_parts[2])\n",
    "                \n",
    "                file_emotion.append(emotion)\n",
    "                file_path.append(os.path.join(actor_path, file))\n",
    "    \n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "\n",
    "# CREMA dataset\n",
    "def load_crema(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "\n",
    "    for file in directory_list:\n",
    "        file_path.append(os.path.join(path, file))\n",
    "        part = file.split('_')[2]\n",
    "\n",
    "        if part == 'SAD':\n",
    "            file_emotion.append('sad')\n",
    "        elif part == 'ANG':\n",
    "            file_emotion.append('angry')\n",
    "        elif part == 'DIS':\n",
    "            file_emotion.append('disgust')\n",
    "        elif part == 'FEA':\n",
    "            file_emotion.append('fear')\n",
    "        elif part == 'HAP':\n",
    "            file_emotion.append('happy')\n",
    "        elif part == 'NEU':\n",
    "            file_emotion.append('neutral')\n",
    "        else:\n",
    "            file_emotion.append('Unknown')\n",
    "\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# TESS dataset\n",
    "def load_tess(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "\n",
    "    for dir in directory_list:\n",
    "        sub_directories = os.listdir(os.path.join(path, dir))\n",
    "        for file in sub_directories:\n",
    "            part = file.split('.')[0].split('_')[2]\n",
    "            emotion = 'surprise' if part == 'ps' else part\n",
    "            file_emotion.append(emotion)\n",
    "            file_path.append(os.path.join(path, dir, file))\n",
    "\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# SAVEE dataset\n",
    "def load_savee(path):\n",
    "    directory_list = os.listdir(path)\n",
    "    file_emotion, file_path = [], []\n",
    "\n",
    "    for file in directory_list:\n",
    "        file_path.append(os.path.join(path, file))\n",
    "        part = file.split('_')[1]\n",
    "        ele = part[:-6]\n",
    "\n",
    "        if ele == 'a':\n",
    "            file_emotion.append('angry')\n",
    "        elif ele == 'd':\n",
    "            file_emotion.append('disgust')\n",
    "        elif ele == 'f':\n",
    "            file_emotion.append('fear')\n",
    "        elif ele == 'h':\n",
    "            file_emotion.append('happy')\n",
    "        elif ele == 'n':\n",
    "            file_emotion.append('neutral')\n",
    "        elif ele == 'sa':\n",
    "            file_emotion.append('sad')\n",
    "        else:\n",
    "            file_emotion.append('surprise')\n",
    "\n",
    "    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "    path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "    return pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(data, sample_rate):\n",
    "    # features: ZCR, Chroma, MFCC, RMS, Mel spectrogram\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=np.abs(librosa.stft(data)), sr=sample_rate).T, axis=0)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "\n",
    "    return np.hstack((zcr, chroma_stft, mfcc, rms, mel))\n",
    "\n",
    "# Load datasets\n",
    "ravdess_df = load_ravdess(RAVDESS_PATH)\n",
    "crema_df = load_crema(CREMA_PATH)\n",
    "tess_df = load_tess(TESS_PATH)\n",
    "savee_df = load_savee(SAVEE_PATH)\n",
    "\n",
    "# Concatenate dataframes\n",
    "data_path = pd.concat([ravdess_df, crema_df, tess_df, savee_df], axis=0)\n",
    "\n",
    "# Feature extraction\n",
    "X, Y = [], []\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    features = extract_features(data, sample_rate)\n",
    "    X.append(features)\n",
    "    Y.append(emotion)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# encode the labels\n",
    "encoder = OneHotEncoder()\n",
    "Y_encoded = encoder.fit_transform(Y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Saving the encoder file\n",
    "joblib.dump(encoder, 'encoder.joblib', protocol=4)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_encoded, random_state=0, shuffle=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Saving the scaler file\n",
    "joblib.dump(scaler, 'scaler.joblib', protocol=4)\n",
    "\n",
    "# Expanding dimensions for compatibility with Conv1D\n",
    "x_train_scaled = np.expand_dims(x_train_scaled, axis=2)\n",
    "x_test_scaled = np.expand_dims(x_test_scaled, axis=2)\n",
    "\n",
    "# Building model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train_scaled.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=len(encoder.categories_[0]), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Callbacks for model training\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
    "checkpoint = ModelCheckpoint(\"speech_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(x_train_scaled, y_train, batch_size=64, epochs=50,\n",
    "                    validation_data=(x_test_scaled, y_test),\n",
    "                    callbacks=[rlrp, checkpoint])\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "accuracy = model.evaluate(x_test_scaled, y_test)[1] * 100\n",
    "print(\"Accuracy of the model on test data:\", accuracy, \"%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
